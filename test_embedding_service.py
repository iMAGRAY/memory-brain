#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ EmbeddingGemma-300M –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
"""

import os
import sys
import time
import torch
import numpy as np
from sentence_transformers import SentenceTransformer

def test_embedding_gemma():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∏ —Ä–∞–±–æ—Ç—ã EmbeddingGemma-300M"""
    
    print("=" * 60)
    print("–¢–ï–°–¢ EMBEDDINGGEMMA-300M –ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
    print("=" * 60)
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ—Ä—Å–∏–π
    print("\n1. –ü–†–û–í–ï–†–ö–ê –í–ï–†–°–ò–ô:")
    print(f"   Python: {sys.version}")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   NumPy: {np.__version__}")
    print(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    
    # 2. –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    print("\n2. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò:")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
    local_path = "C:/Models/ai-memory-service/models/embeddinggemma-300m"
    import os
    if os.path.exists(local_path):
        print(f"   ‚úì –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞: {local_path}")
        model_path = local_path
    else:
        print(f"   ‚ö† –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º HuggingFace")
        model_path = "google/embeddinggemma-300m"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    if device == 'cuda':
        try:
            gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_free = (torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3
            print(f"   GPU –ø–∞–º—è—Ç—å: {gpu_free:.1f}GB —Å–≤–æ–±–æ–¥–Ω–æ –∏–∑ {gpu_mem:.1f}GB")
            if gpu_free < 2.0:
                print(f"   ‚ö† –ú–∞–ª–æ GPU –ø–∞–º—è—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                device = 'cpu'
        except Exception:
            pass
    
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º device: {device}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print(f"   –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å: {model_path}")
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–≥—Ä—É–∑–∫–∏
        model = None
        dtype_used = None
        
        # –°–ø–æ—Å–æ–± 1: –ü–æ–ø—Ä–æ–±—É–µ–º —Å model_kwargs
        try:
            print("   –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å model_kwargs –¥–ª—è bfloat16...")
            model = SentenceTransformer(
                model_path,
                device=device,
                model_kwargs={'torch_dtype': torch.bfloat16}
            )
            dtype_used = "bfloat16 (—á–µ—Ä–µ–∑ model_kwargs)"
            print("   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å model_kwargs!")
        except (TypeError, ValueError) as e1:
            print(f"   ‚ö† model_kwargs –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è: {type(e1).__name__}")
            
        # –°–ø–æ—Å–æ–± 2: –ó–∞–≥—Ä—É–∑–∫–∞ –±–µ–∑ dtype –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ø–æ—Å–ª–µ
        if model is None:
            try:
                print("   –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏...")
                model = SentenceTransformer(model_path, device=device)
                print("   ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å—é –º–æ–¥–µ–ª—å –≤ bfloat16 –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
                if device == 'cuda' and torch.cuda.is_bf16_supported():
                    print("   –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –≤ bfloat16...")
                    model = model.to(torch.bfloat16)
                    dtype_used = "bfloat16 (–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏)"
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é
                    if hasattr(model, 'parameters'):
                        try:
                            params = list(model.parameters())
                            if params:
                                first_param_dtype = params[0].dtype
                                if first_param_dtype == torch.bfloat16:
                                    print("   ‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ bfloat16!")
                                else:
                                    print(f"   ‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: dtype={first_param_dtype}, –æ–∂–∏–¥–∞–ª—Å—è bfloat16")
                                    dtype_used = f"{first_param_dtype} (–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å)"
                            else:
                                print("   ‚ö† –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ dtype")
                        except Exception as e:
                            print(f"   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å dtype: {e}")
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –¥–ª—è CPU –∏–ª–∏ –µ—Å–ª–∏ bfloat16 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
                    model = model.to(torch.float32)
                    dtype_used = "float32 (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π)"
                    print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 (device={device})")
                    
            except (OSError, ValueError) as e2:
                print(f"   ‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {type(e2).__name__}: {e2}")
                raise
            except AssertionError as e3:
                print(f"   ‚ö† –ü—Ä–æ–±–ª–µ–º–∞ —Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π dtype: {e3}")
                dtype_used = "float32 (fallback)"
        
        if model is None:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∏ –æ–¥–Ω–∏–º —Å–ø–æ—Å–æ–±–æ–º")
            
        print(f"   ‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        print(f"   –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–π dtype: {dtype_used}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
        if hasattr(model, 'parameters'):
            param_count = sum(p.numel() for p in model.parameters())
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {param_count/1e6:.1f}M")
        
    except ImportError as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
        print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É sentence-transformers")
        return False
    except OSError as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª–∞–º –º–æ–¥–µ–ª–∏: {e}")
        print(f"   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: {model_path}")
        return False
    except RuntimeError as e:
        print(f"   ‚úó Runtime –æ—à–∏–±–∫–∞: {e}")
        print("   –í–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–±–ª–µ–º–∞ —Å CUDA, –ø–∞–º—è—Ç—å—é –∏–ª–∏ —Ñ–æ—Ä–º–∞—Ç–æ–º –º–æ–¥–µ–ª–∏")
        return False
    except ValueError as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {e}")
        return False
    except Exception as e:
        print(f"   ‚úó –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__}: {e}")
        return False
    
    # 3. –¢–µ—Å—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤
    print("\n3. –¢–ï–°–¢ –ü–†–û–ú–ü–¢–û–í (—Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏):")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
    test_texts = {
        "query": "machine learning algorithms",
        "document": "Machine learning is a subset of artificial intelligence that enables systems to learn from data"
    }
    
    # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è EmbeddingGemma
    prompts = {
        "query": f"task: search result | query: {test_texts['query']}",
        "document": f"title: none | text: {test_texts['document']}"
    }
    
    print(f"   Query prompt: {prompts['query'][:50]}...")
    print(f"   Document prompt: {prompts['document'][:50]}...")
    
    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("\n4. –ì–ï–ù–ï–†–ê–¶–ò–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í:")
    
    try:
        start_time = time.time()
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏
        query_embedding = model.encode([prompts["query"]], normalize_embeddings=True)
        doc_embedding = model.encode([prompts["document"]], normalize_embeddings=True)
        
        elapsed = time.time() - start_time
        
        print(f"   ‚úì –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {elapsed:.2f} —Å–µ–∫")
        print(f"   Query embedding shape: {query_embedding.shape}")
        print(f"   Document embedding shape: {doc_embedding.shape}")
        print(f"   –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {query_embedding.shape[1]} (–æ–∂–∏–¥–∞–µ—Ç—Å—è 768)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        assert query_embedding.shape[1] == 768, f"–ù–µ–≤–µ—Ä–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {query_embedding.shape[1]}"
        
    except Exception as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return False
    
    # 5. –¢–µ—Å—Ç Matryoshka dimensions
    print("\n5. –¢–ï–°–¢ MATRYOSHKA DIMENSIONS:")
    
    supported_dims = [768, 512, 256, 128]
    for dim in supported_dims:
        try:
            # –£—Å–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            truncated = query_embedding[:, :dim]
            print(f"   ‚úì Dimension {dim}: shape {truncated.shape}")
        except Exception as e:
            print(f"   ‚úó Dimension {dim}: –æ—à–∏–±–∫–∞ {e}")
    
    # 6. –¢–µ—Å—Ç batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
    print("\n6. –¢–ï–°–¢ BATCH –û–ë–†–ê–ë–û–¢–ö–ò:")
    
    batch_texts = [
        "task: search result | query: python programming",
        "task: search result | query: data science",
        "task: search result | query: deep learning",
        "title: none | text: Python is a high-level programming language",
        "title: none | text: Data science combines statistics and computing"
    ]
    
    try:
        start_time = time.time()
        batch_embeddings = model.encode(batch_texts, normalize_embeddings=True)
        elapsed = time.time() - start_time
        
        print(f"   ‚úì Batch –∏–∑ {len(batch_texts)} —Ç–µ–∫—Å—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫")
        print(f"   Batch shape: {batch_embeddings.shape}")
        
    except Exception as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
        return False
    
    # 7. –¢–µ—Å—Ç similarity
    print("\n7. –¢–ï–°–¢ SIMILARITY:")
    
    try:
        # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É query –∏ document
        from numpy.linalg import norm
        
        q_emb = query_embedding[0]
        d_emb = doc_embedding[0]
        
        cosine_sim = np.dot(q_emb, d_emb) / (norm(q_emb) * norm(d_emb))
        print(f"   Cosine similarity (query vs document): {cosine_sim:.4f}")
        
        # Similarity –º–µ–∂–¥—É –ø–æ—Ö–æ–∂–∏–º–∏ queries
        q1 = model.encode(["task: search result | query: machine learning"], normalize_embeddings=True)[0]
        q2 = model.encode(["task: search result | query: ML algorithms"], normalize_embeddings=True)[0]
        
        sim_similar = np.dot(q1, q2) / (norm(q1) * norm(q2))
        print(f"   Cosine similarity (–ø–æ—Ö–æ–∂–∏–µ queries): {sim_similar:.4f}")
        
    except Exception as e:
        print(f"   ‚úó –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è similarity: {e}")
        return False
    
    # 8. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\n8. BENCHMARK –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
    
    test_sizes = [1, 10, 50]
    for size in test_sizes:
        texts = [f"task: search result | query: test query {i}" for i in range(size)]
        
        start_time = time.time()
        _ = model.encode(texts, normalize_embeddings=True)
        elapsed = time.time() - start_time
        
        throughput = size / elapsed
        print(f"   Batch size {size:3d}: {elapsed:.3f} —Å–µ–∫ ({throughput:.1f} texts/sec)")
    
    print("\n" + "=" * 60)
    print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´ –£–°–ü–ï–®–ù–û!")
    print("=" * 60)
    
    return True

def test_dtype_compatibility():
    """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ dtype —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ DTYPE –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–ò")
    print("=" * 60)
    
    model_path = "C:/Models/ai-memory-service/models/embeddinggemma-300m"
    if not os.path.exists(model_path):
        model_path = "google/embeddinggemma-300m"
    
    # –¢–µ—Å—Ç —Ä–∞–∑–ª–∏—á–Ω—ã—Ö dtype
    dtypes_to_test = [
        ("bfloat16", torch.bfloat16, True),   # –î–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å
        ("float32", torch.float32, True),      # –î–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å
        # ("float16", torch.float16, False),   # –ù–ï –¥–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å - –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å
    ]
    
    for dtype_name, dtype, should_work in dtypes_to_test:
        print(f"\n–¢–µ—Å—Ç dtype: {dtype_name}")
        try:
            model = SentenceTransformer(model_path, torch_dtype=dtype)
            test_emb = model.encode(["test"], normalize_embeddings=True)
            print(f"   ‚úì {dtype_name}: —Ä–∞–±–æ—Ç–∞–µ—Ç! Shape: {test_emb.shape}")
        except Exception as e:
            if should_work:
                print(f"   ‚úó {dtype_name}: –æ—à–∏–±–∫–∞ (–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ): {e}")
            else:
                print(f"   ‚úì {dtype_name}: –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç (–æ–∂–∏–¥–∞–µ–º–æ): {type(e).__name__}")

if __name__ == "__main__":
    print("–ó–ê–ü–£–°–ö –¢–ï–°–¢–û–í AI MEMORY SERVICE - EMBEDDINGGEMMA INTEGRATION")
    print("Python:", sys.version)
    print("-" * 60)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã
    success = test_embedding_gemma()
    
    # –¢–µ—Å—Ç dtype
    if success:
        test_dtype_compatibility()
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
    if success:
        print("\nüéâ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –†–ê–ë–û–¢–ê–ï–¢ –ö–û–†–†–ï–ö–¢–ù–û!")
        sys.exit(0)
    else:
        print("\n‚ùå –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´ –í –ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
        sys.exit(1)