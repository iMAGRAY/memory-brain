# EmbeddingGemma-300M Model Configuration
# Production configuration for AI Memory Service

[model]
# Model identification
name = "embeddinggemma-300m"
huggingface_id = "google/embeddinggemma-300m"
version = "300m"

# Model paths (in order of priority)
# 1. Environment variable: EMBEDDINGGEMMA_MODEL_PATH
# 2. Local paths checked automatically:
local_search_paths = [
    "./models/embeddinggemma-300m",
    "../models/embeddinggemma-300m",
    "~/.cache/models/embeddinggemma-300m",
    "/opt/models/embeddinggemma-300m"
]

# Required model files for validation
required_files = [
    "model.safetensors",
    "config.json", 
    "tokenizer.json",
    "tokenizer_config.json",
    "config_sentence_transformers.json"
]

# Minimum model file size (bytes) for validation
min_model_size = 1000000  # 1MB minimum

[embedding]
# Embedding dimensions (Matryoshka: 768 native, service uses 512 by default)
default_dimension = 512
matryoshka_dimensions = [768, 512, 256, 128]

# Context settings
max_sequence_length = 2048
vocab_size = 256000

# Task-specific prompts for optimal performance
[prompts]
query = "task: search result | query: {}"
document = "title: none | text: {}"
classification = "task: classification | text: {}"
similarity = "task: similarity | text: {}"

[performance]
# Device configuration
# Options: "cpu", "cuda", "cuda:0", "cuda:1", "auto"
# Can be overridden by EMBEDDING_DEVICE environment variable
default_device = "cpu"

# Data type configuration for EmbeddingGemma
# CRITICAL: EmbeddingGemma does NOT support float16
# Options: "bfloat16" (recommended), "float32"
# Using float32 for better compatibility and stability
torch_dtype = "float32"

# Batch processing
default_batch_size = 8
max_batch_size = 128

# Timeout settings (seconds)
embedding_timeout = 30

# Cache configuration
[cache]
enabled = true
ttl_seconds = 3600  # 1 hour
max_entries = 10000
eviction_strategy = "lru"  # least recently used

# Input validation
[validation]
max_text_length = 8192
max_total_batch_chars = 1048576  # 1MB
sanitize_input = true
allow_empty_text = true

# Logging configuration
[logging]
level = "info"  # Options: "trace", "debug", "info", "warn", "error"
log_cache_hits = true
log_model_loading = true
log_performance_metrics = false
