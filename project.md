# AI Memory Service - Техническое описание проекта

## Что это за проект

**AI Memory Service** - это система долговременной памяти для AI-агентов, которая позволяет им помнить контекст между сессиями и находить релевантную информацию из прошлых взаимодействий.

## Главная цель

Дать AI-агентам способность:
1. **Запоминать** - сохранять важную информацию из разговоров
2. **Вспоминать** - находить нужные воспоминания по смыслу
3. **Учиться** - улучшать свои ответы на основе прошлого опыта

## Как это должно работать

### 1. Сохранение памяти
```
Пользователь: "Мой любимый язык программирования - Rust"
    ↓
AI Memory Service сохраняет:
- Текст: "Пользователь предпочитает Rust"
- Вектор: [0.23, -0.45, 0.67, ...] (512 измерений)
- Метаданные: {важность: 0.8, контекст: "preferences", время: "2025-09-09"}
    ↓
Neo4j база данных
```

### 2. Поиск по памяти
```
Новый вопрос: "На каком языке мне написать системную утилиту?"
    ↓
Embedding: превращаем вопрос в вектор
    ↓
SIMD поиск: находим похожие векторы в памяти
    ↓
Результат: "Вы предпочитаете Rust для системного программирования"
```

### 3. Автоматическая оптимизация
```
Каждую ночь:
- Объединяем похожие воспоминания
- Удаляем неважные
- Выделяем ключевые паттерны
```

## Технические компоненты

### Embedding Server (Python)
**Что делает:** Превращает текст в векторы  
**Модель:** EmbeddingGemma-300M (512 dimensions)  
**Порт:** 8090  
**API:**
```python
POST /embed
{
    "text": "любой текст",
    "task_type": "document" | "query"
}
→ {"embedding": [0.1, -0.2, ...], "dimension": 512}
```

### Memory Server (Rust)
**Что делает:** Управляет сохранением и поиском памяти  
**База:** Neo4j (граф для связей между воспоминаниями)  
**Порт:** 8080  
**API:**
```rust
POST /api/memory - создать воспоминание
GET /api/memory/search - найти похожие
POST /api/memory/consolidate - оптимизировать
```

### SIMD Search (Rust)
**Что делает:** Супербыстрый поиск похожих векторов  
**Технология:** AVX2/AVX512 инструкции процессора  
**Скорость:** 1M векторов за 10ms  

### GPT Orchestrator
**Что делает:** Решает что важно запомнить  
**Модель:** GPT-5-nano (или GPT-4)  
**Функции:**
- Оценка важности информации
- Категоризация воспоминаний
- Извлечение ключевых фактов

## Конкретные use cases

### 1. Персональный ассистент
```
День 1: "Завтра у меня встреча с Иваном в 15:00"
День 7: "Когда у меня встреча с Иваном?"
AI: "У вас была встреча с Иваном неделю назад, 2 сентября в 15:00"
```

### 2. Техподдержка
```
Клиент #1: "Ошибка при запуске на Windows 11"
Клиент #2: "Программа не запускается"
AI: "Похожая проблема была у другого клиента на Windows 11, решение..."
```

### 3. Обучающий бот
```
Урок 1: "Переменные в Python объявляются без типа"
Урок 10: "Напомни про переменные"
AI: "Как мы изучали в уроке 1, в Python переменные..."
```

## Текущее состояние

### ✅ Что работает:
- Embedding сервер генерирует векторы
- Memory сервер запускается и принимает запросы
- Neo4j подключена и готова хранить данные
- SIMD оптимизации скомпилированы

### ❌ Что нужно починить:
1. **API несоответствие** - embedding ожидает `text`, тесты шлют `texts`
2. **Создание памяти** - endpoint возвращает пустой ответ
3. **Batch обработка** - нет поддержки массовых embeddings
4. **Orchestrator не скомпилирован** - нужно `cargo build --release --bin orchestrator`

## Как запустить сейчас

```bash
# 1. Embedding сервер
python embedding_server.py

# 2. Memory сервер  
./target/release/memory-server

# 3. Проверка
curl http://localhost:8090/health
curl http://localhost:8080/health

# 4. Создать embedding
curl -X POST http://localhost:8090/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "test", "task_type": "document"}'
```

## Что нужно сделать для полной работы

### Шаг 1: Скомпилировать недостающее
```bash
cargo build --release --bin orchestrator
cargo build --release --bin memory_organizer
```

### Шаг 2: Исправить API
В `comprehensive_test.py` поменять:
- `"texts"` → `"text"` 
- Убрать batch логику или добавить её в сервер

### Шаг 3: Настроить окружение
```bash
export NEO4J_PASSWORD=your_password
export OPENAI_API_KEY=sk-...
export MODEL_PATH=./models/embeddinggemma-300m
```

### Шаг 4: Запустить всё вместе
```bash
docker-compose up  # когда всё починим
```

## Ожидаемый результат

Система, которая:
1. **Работает** - все компоненты запускаются без ошибок
2. **Запоминает** - сохраняет текст → вектор → Neo4j
3. **Находит** - поиск по смыслу < 100ms на 1M записей
4. **Оптимизирует** - автоматическая консолидация памяти
5. **Масштабируется** - Docker + Kubernetes ready

## Производительность (целевая)

- **Embedding latency:** < 50ms для одного текста
- **Search latency:** < 100ms для 1M векторов  
- **Memory footprint:** < 1GB для 1M воспоминаний
- **Throughput:** 1000 запросов/сек
- **Accuracy:** 95% recall@10 для семантического поиска

## Интеграция

Система предоставляет REST API для интеграции с:
- ChatGPT/Claude (через custom tools)
- Telegram/Discord ботами
- Web приложениями
- Другими AI сервисами

## Безопасность

- Все credentials в переменных окружения
- JWT токены для API аутентификации
- Шифрование sensitive воспоминаний
- Rate limiting для защиты от DDoS

---

**Это не просто база данных чатов.** Это система, которая даёт AI настоящую долговременную память с пониманием контекста и способностью учиться на опыте.