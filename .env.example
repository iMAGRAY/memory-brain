# AI Memory Service - Environment Configuration Template
# Copy this file to .env and set your actual values
# SECURITY: Never commit .env file with real credentials to version control

# ============================================================================
# REQUIRED CREDENTIALS (Must be set)
# ============================================================================

# Neo4j Database Password (REQUIRED)
# Generate a strong password for production use
NEO4J_PASSWORD=your_strong_password_here

# OpenAI API Key for GPT Orchestrator (REQUIRED)
# Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your_openai_api_key_here

# Model Path for Embedding Server (REQUIRED)
# Path to your EmbeddingGemma-300M model directory
MODEL_PATH=./models/embeddinggemma-300m
MODELS_PATH=./models

# ============================================================================
# SERVICE CONFIGURATION
# ============================================================================

# Environment and Logging
ENVIRONMENT=production
RUST_LOG=info,ai_memory_service=debug
RUST_BACKTRACE=1

# Network Configuration
SERVICE_HOST=0.0.0.0
SERVICE_PORT=8080
WORKERS=4

# SECURITY: Specific CORS origins (no wildcards for production)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080,https://yourdomain.com

# Neo4j Configuration
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_POOL_SIZE=10

# ============================================================================
# EMBEDDING SERVER CONFIGURATION (NEW HTTP-based Architecture)
# ============================================================================

# Embedding Server Connection
EMBEDDING_SERVER_URL=http://embedding-server:8090
EMBEDDING_TIMEOUT=30
EMBEDDING_MAX_CONCURRENT=4

# Embedding Server Settings
EMBEDDING_MAX_WORKERS=4
EMBEDDING_BATCH_SIZE=8
MAX_SEQUENCE_LENGTH=2048
EMBEDDING_LOG_LEVEL=INFO

# =============================================================================
# Optional: Cache Configuration
# =============================================================================

# L1 Cache (Hot memories in memory)
L1_CACHE_SIZE=1000

# L2 Cache (Warm memories with TTL)
L2_CACHE_SIZE=10000
CACHE_TTL=3600
CACHE_COMPRESSION=true

# =============================================================================
# Optional: AI Brain Configuration
# =============================================================================

# Memory Management
MAX_MEMORIES=100000
IMPORTANCE_THRESHOLD=0.3
CONSOLIDATION_INTERVAL=300
MEMORY_DECAY_RATE=0.01

# =============================================================================
# Optional: GPT Orchestrator Configuration
# =============================================================================

# Model Selection (gpt-4-turbo, gpt-4, gpt-3.5-turbo)
ORCHESTRATOR_MODEL=gpt-4-turbo

# Token Limits
MAX_INPUT_TOKENS=400000
MAX_OUTPUT_TOKENS=12000

# Generation Parameters
ORCHESTRATOR_TEMPERATURE=1.0
ORCHESTRATOR_TIMEOUT=120

# =============================================================================
# Optional: Monitoring (only needed if using --profile monitoring)
# =============================================================================

# Grafana Admin Password
# Generate: openssl rand -base64 16
GRAFANA_PASSWORD=your_grafana_password_here

# =============================================================================
# Advanced: Performance Tuning
# =============================================================================

# Resource Limits (for development/testing)
# Uncomment and adjust based on your system capabilities
# AI_MEMORY_MAX_MEMORY=8G
# AI_MEMORY_MAX_CPU=2.0
# NEO4J_MAX_MEMORY=2G
# NEO4J_MAX_CPU=1.0

# =============================================================================
# Development Settings (optional)
# =============================================================================

# Enable debug features in development
# DEBUG_MODE=false
# ENABLE_METRICS=true
# LOG_REQUESTS=false