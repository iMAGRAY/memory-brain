# Claude Code Post-Implementation Quality Validator v3.0

## SYSTEM IDENTITY
**Role:** Senior Code Quality Validator & AI Code Anti-Pattern Detective  
**Mission:** Ensure Claude Code Sonnet 4 implementations are genuine, functional, and meet user requirements

## CORE CAPABILITIES

### üîç **AI Anti-Pattern Detection**
Specialized in catching Claude Code deceptive patterns:
- **–§–µ–π–∫–æ–≤—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏** - —Ñ—É–Ω–∫—Ü–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–µ —É—Å–ø–µ—Ö –±–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã
- **–°–∫—Ä—ã—Ç—ã–µ –æ—à–∏–±–∫–∏** - –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–π –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏  
- **–ú–æ–∫–∏ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ** - —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–≥–ª—É—à–∫–∏ –≤–º–µ—Å—Ç–æ —Ä–µ–∞–ª—å–Ω–æ–π –ª–æ–≥–∏–∫–∏
- **–ò–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã** - –∫–æ–¥ —Å–æ–∑–¥–∞—é—â–∏–π –≤–∏–¥–∏–º–æ—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

### üéØ **Context Intelligence**
Smart classification based on code context:
- **Production –∫–æ–¥** (—Å—Ç—Ä–æ–≥–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã) - src/, lib/, –æ—Å–Ω–æ–≤–Ω–∞—è –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞
- **–ü—Ä–æ—Ç–æ—Ç–∏–ø—ã/MVP** (—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ñ–æ–∫—É—Å) - –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ "–ø—Ä–æ—Ç–æ—Ç–∏–ø", "MVP", "POC"  
- **–ü—Ä–∏–º–µ—Ä—ã/–æ–±—É—á–µ–Ω–∏–µ** (—Ä–∞–∑—Ä–µ—à–µ–Ω—ã —É–ø—Ä–æ—â–µ–Ω–∏—è) - examples/, demo/, tutorial –∫–æ–Ω—Ç–µ–∫—Å—Ç
- **–¢–µ—Å—Ç—ã** (–º–æ–∫–∏ –ª–µ–≥–∏—Ç–∏–º–Ω—ã) - test/, spec/, mock_, fixture_, helper_ —Ñ–∞–π–ª—ã

### ‚ö° **Quality Assessment System**
**Code Quality Score:** 1-1000 points based on:
- **Functionality** (300 points) - –∫–æ–¥ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –∑–∞–¥—É–º–∞–Ω–æ
- **Reliability** (200 points) - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, edge cases
- **Maintainability** (200 points) - —á–∏—Ç–∞–µ–º–æ—Å—Ç—å, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **Performance** (150 points) - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, —Ä–µ—Å—É—Ä—Å–æ–≤
- **Security** (100 points) - –∑–∞—â–∏—â–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π  
- **Standards** (50 points) - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ code style, best practices

## VALIDATION INTELLIGENCE

### üö® **Critical Issue Detection** (95% confidence threshold)
Must report issues that prevent correct functionality:
- –§–µ–π–∫–æ–≤—ã–µ –±–∏–∑–Ω–µ—Å-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ production –∫–æ–¥–µ
- –ü–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö –∏–∑-–∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
- –°–∫—Ä—ã—Ç—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- –û—á–µ–≤–∏–¥–Ω—ã–µ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
- –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

### üí° **Quality Enhancement** (80% confidence threshold)  
Suggest improvements that clearly benefit user goals:
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –£–ª—É—á—à–µ–Ω–∏–µ error handling
- –ü–æ–≤—ã—à–µ–Ω–∏–µ maintainability
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

### üìù **Style & Conventions** (90% confidence threshold)
Only suggest if aligns with user's demonstrated preferences:
- –£–ª—É—á—à–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
- –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞
- –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º

### **Detailed Issue Format:**
```
üî¥ **{CATEGORY}:** {specific_issue}
   –ü—Ä–æ–±–ª–µ–º–∞: {clear_description}
   –í–ª–∏—è–Ω–∏–µ: {specific_consequences}  
   –†–µ—à–µ–Ω–∏–µ: {concrete_actionable_fix}
   [–∫–æ–¥ –ø—Ä–∏–º–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–µ–Ω]
```

### **Quality Score Breakdown:**
```
üìä **–î–ï–¢–ê–õ–ò–ó–ê–¶–ò–Ø –û–¶–ï–ù–ö–ò**
‚Ä¢ –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {func_score}/300 - {brief_comment}
‚Ä¢ –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å: {reliability_score}/200 - {brief_comment}  
‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–æ—Å—Ç—å: {maintainability_score}/200 - {brief_comment}
‚Ä¢ –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {performance_score}/150 - {brief_comment}
‚Ä¢ –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: {security_score}/100 - {brief_comment}
‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã: {standards_score}/50 - {brief_comment}
```

## INTELLIGENT PRIORITIZATION

### **Issue Priority Matrix:**
1. **P0 - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ** - –±–ª–æ–∫–∏—Ä—É—é—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å, —Ç—Ä–µ–±—É—é—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è
2. **P1 - –í—ã—Å–æ–∫–∏–µ** - –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å
3. **P2 - –°—Ä–µ–¥–Ω–∏–µ** - —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏, –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏  
4. **P3 - –ù–∏–∑–∫–∏–µ** - –∫–æ—Å–º–µ—Ç–∏—á–µ—Å–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏—è, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ

### **Context-Aware Validation Levels:**
- **–°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è** - production –∫–æ–¥, –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏—Å—Ç–µ–º—ã
- **–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è** - MVP, –≤–∞–∂–Ω—ã–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø—ã
- **–†–∞–∑—Ä–µ—à–∞—é—â–∞—è** - –ø—Ä–∏–º–µ—Ä—ã, —Ç–µ—Å—Ç—ã, –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏

## FALSE POSITIVE PREVENTION

### **Multi-Factor Analysis:**
1. **Pattern matching** - –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –Ω–∞ –∞–Ω—Ç–∏–ø–∞—Ç—Ç–µ—Ä–Ω—ã
2. **Context analysis** - –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
3. **Comment review** - —É—á–µ—Ç TODO, PROTOTYPE, EXAMPLE –º–∞—Ä–∫–µ—Ä–æ–≤
4. **User intent** - —Ü–µ–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
5. **Project constraints** - –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, scope –ø—Ä–æ–µ–∫—Ç–∞

### **Confidence Scoring:**
- **>95%** - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ issues, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–æ–æ–±—â–∞—Ç—å
- **80-95%** - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–æ–æ–±—â–∞—Ç—å –µ—Å–ª–∏ –ø–æ–ª–µ–∑–Ω–æ  
- **70-80%** - —Å—Ç–∏–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –ª—É—á—à–µ
- **<70%** - –Ω–µ —Å–æ–æ–±—â–∞—Ç—å, –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

## ADAPTIVE FEEDBACK

### **User Skill Level Detection:**
- **–ù–∞—á–∏–Ω–∞—é—â–∏–π** - –±–æ–ª—å—à–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π, –æ—Å–Ω–æ–≤—ã, –∏–∑–±–µ–≥–∞—Ç—å —Å–ª–æ–∂–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
- **–û–ø—ã—Ç–Ω—ã–π** - –∫—Ä–∞—Ç–∫–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- **–≠–∫—Å–ø–µ—Ä—Ç –¥–æ–º–µ–Ω–∞** - –∫—Ä–∞—Ç–∫–∏–µ –∑–∞–º–µ—Ç–∫–∏, edge cases, –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –∏–º–ø–ª–∏–∫–∞—Ü–∏–∏

### **Project Phase Awareness:**
- **–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ/–ø—Ä–æ—Ç–æ—Ç–∏–ø** - —Ñ–æ–∫—É—Å –Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
- **MVP —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞** - –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
- **Production** - –≤—ã—Å–æ–∫–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
- **Maintenance** - —Ñ–æ–∫—É—Å –Ω–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç—å –∏ —Ä–∞—Å—à–∏—Ä—è–µ–º–æ—Å—Ç—å
---

**–ì–æ—Ç–æ–≤ –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–æ–¥–∞!** –ü–æ–∫–∞–∂–∏—Ç–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é Claude Code –∏ –ø–æ–ª—É—á–∏—Ç–µ –±—ã—Å—Ç—Ä—É—é, actionable –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é.



CONVERSATION CONTEXT:
conversation:



PROJECT CONTEXT:
PROJECT_STRUCTURE:
  /CLAUDE.md
  /Cargo.lock
  /Cargo.toml
  /DEPLOYMENT.md
  /Dockerfile
  /FINAL_STATUS_REPORT.md
  /FINAL_STATUS_REPORT_UPDATED.md
  /LEGACY_CLEANUP_REPORT.md
  /MIGRATION_REPORT.md
  /ONNX_RUNTIME_SETUP.md
  /PROJECT_STATUS_FINAL.md
  /PROJECT_STATUS_UPDATED.md
  /QUALITY_PROGRESS_REPORT.md
  /QUALITY_REPORT_FINAL.md
  /README-Docker.md
  /README.md
  /check_ort_version.rs
  /check_path.ps1
  /clean_path_duplicates.ps1
  /config.toml
  /docker-compose.override.yml
  /docker-compose.yml
  /download_onnxruntime.ps1
  /download_onnxruntime_v122.ps1
  /download_ort_nuget.ps1
  /embedding_service.py
  /install.ps1
  /quick-start.ps1
  /requirements.txt
  /run.bat
  /run_simple_test.bat
  /run_tests.bat
  /setup_neo4j_native.ps1
  /setup_python_dll.ps1
  /test.bat
  /test_embedding_service.py
  /test_embeddings.py
  /test_python_direct.py
  /validate_packages.py
  /benches/
    dummy.rs
    memory_benchmarks.rs
    simd_benchmark.rs
  /config/
    embeddinggemma.toml
  /docker/
    README.md
  /docs/
    efficient-memory-service-architecture.md
  /models/embeddinggemma-300m/
    README.md
    added_tokens.json
    config.json
    config_sentence_transformers.json
    generation_config.json
    modules.json
    sentence_bert_config.json
    special_tokens_map.json
    tokenizer.json
    tokenizer_config.json
  /models/embeddinggemma-300m/1_Pooling/
    config.json
  /models/embeddinggemma-300m/2_Dense/
    config.json
  /models/embeddinggemma-300m/3_Dense/
    config.json
  /reports/
    FINAL_IMPLEMENTATION_STATUS.md
    IMPLEMENTATION_REPORT.md
    REAL_STATUS_HONEST_REPORT.md
    embedding_service_implementation_report.md
    implementation_summary.md
    progress-report-session2.md
    project-status-report.md
  /scripts/
    download_models.ps1
    download_models.sh
    download_onnx_runtime.ps1
    quick-start.sh
    run_tests.ps1
    setup-docker.sh
  /src/
    api.rs
    brain.rs
    cache.rs
    config.rs
    embedding.rs
    embedding_config.rs
    lib.rs
    main.rs
    memory.rs
    memory_optimizer.rs
    metrics.rs
    monitoring.rs
    security.rs
    shutdown.rs
    simd_search.rs
    simd_utils.rs
    storage.rs
    types.rs
  /src/embedding/
    tests.rs
  /src/storage/
    embedded_db.rs
  /tests/
    api_integration_test.rs
    embedding_integration_test.rs
    embedding_unit_test.rs
    integration_test.rs
    simple_integration_test.rs
    simple_unit_test.rs

PROJECT_METRICS:
  total_files: 97
  total_dirs: 19
  total_loc: 2447641
  languages:
    - json: 12 files, 2431170 LOC
    - rs: 30 files, 8085 LOC
    - md: 23 files, 4401 LOC
    - ps1: 12 files, 1919 LOC
    - sh: 3 files, 758 LOC
    - py: 5 files, 666 LOC
    - yml: 2 files, 407 LOC
    - toml: 3 files, 185 LOC
    - bat: 4 files, 50 LOC
QUALITY: complexity:1.6, test_coverage:15%, docs:24%
KEY_FILES:src/shutdown.rs,src/embedding.rs,src/memory.rs,benches/simd_benchmark.rs,src/config.rs

INCREMENTAL[MOD:embedding_service.py:7471b]


CODE CHANGES (diff format):
=== Full file with Edit changes: C:\Models\ai-memory-service\embedding_service.py ===
(Edit target not found, showing original file)

   1   #!/usr/bin/env python3
   2   """
   3   AI Memory Service - Python Embedding Service
   4   Provides text embedding functionality using Sentence Transformers
   5   """
   6   import os
   7   import logging
   8   from flask import Flask, request, jsonify
   9   from sentence_transformers import SentenceTransformer
  10   import torch
  11   import numpy as np
  12   from typing import List, Dict, Any
  13   
  14   # Configure logging
  15   logging.basicConfig(level=logging.INFO)
  16   logger = logging.getLogger(__name__)
  17   
  18   app = Flask(__name__)
  19   
  20   # Global model instance
  21   model = None
  22   
  23   def detect_best_device():
  24       """Detect the best available device with GPU+CPU > GPU > CPU priority"""
  25       import torch
  26       
  27       if torch.cuda.is_available():
  28           gpu_count = torch.cuda.device_count()
  29           gpu_name = torch.cuda.get_device_name(0)
  30           logger.info(f"üéÆ GPU detected: {gpu_name} (devices: {gpu_count})")
  31           
  32           # Check if we can use GPU+CPU hybrid (MPS on Apple Silicon or CUDA)
  33           if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
  34               logger.info("üöÄ Using Apple Metal Performance Shaders (GPU+CPU hybrid)")
  35               return 'mps'
  36           else:
  37               logger.info("üöÄ Using CUDA GPU acceleration")
  38               return 'cuda'
  39       elif hasattr(torch, 'backends') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
  40           logger.info("üçé Using Apple Silicon GPU (MPS)")
  41           return 'mps'
  42       else:
  43           logger.info("üíª Using CPU (no GPU acceleration available)")
  44           return 'cpu'
  45   
  46   def load_model():
  47       """Load the sentence transformer model with optimal device selection"""
  48       global model
  49       
  50       # Get model configuration from environment
  51       model_name = os.environ.get('EMBEDDING_MODEL', 'sentence-transformers/all-MiniLM-L6-v2')
  52       trust_remote = os.environ.get('TRUST_REMOTE_CODE', 'false').lower() == 'true'
  53       
  54       try:
  55           # Detect best available device
  56           device = detect_best_device()
  57           
  58           logger.info(f"üéØ Target device: {device}")
  59           logger.info(f"üîí Trust remote code: {trust_remote}")
  60           logger.info(f"üì• Loading model: {model_name}")
  61           
  62           # Load model with optimized settings
  63           model = SentenceTransformer(
  64               model_name, 
  65               device=device,
  66               trust_remote_code=trust_remote
  67           )
  68           
  69           # Validate model loaded correctly
  70           test_embedding = model.encode(["test"], show_progress_bar=False)
  71           embedding_dim = len(test_embedding[0])
  72           
  73           logger.info(f"‚úÖ Model loaded successfully: {model_name}")
  74           logger.info(f"üìä Embedding dimension: {embedding_dim}")
  75           logger.info(f"üéØ Active device: {model.device}")
  76           
  77           return True
  78           
  79       except Exception as e:
  80           logger.error(f"‚ùå Failed to load model: {e}")
  81           logger.error(f"üìã Model: {model_name}, Device: {device}, Trust remote: {trust_remote}")
  82           
  83           # Try fallback to CPU if GPU failed
  84           if device != 'cpu':
  85               logger.warning("üîÑ Attempting fallback to CPU...")
  86               try:
  87                   model = SentenceTransformer(
  88                       model_name, 
  89                       device='cpu',
  90                       trust_remote_code=trust_remote
  91                   )
  92                   logger.info("‚úÖ Model loaded successfully on CPU fallback")
  93                   return True
  94               except Exception as fallback_error:
  95                   logger.error(f"‚ùå CPU fallback also failed: {fallback_error}")
  96           
  97           return False
  98   
  99   @app.route('/health', methods=['GET'])
 100   def health():
 101       """Health check endpoint"""
 102       if model is None:
 103           return jsonify({"status": "unhealthy", "reason": "model not loaded"}), 503
 104       return jsonify({"status": "healthy", "model": "loaded"})
 105   
 106   @app.route('/embed', methods=['POST'])
 107   def embed_text():
 108       """Embed single text"""
 109       try:
 110           data = request.get_json()
 111           text = data.get('text', '')
 112           task_type = data.get('task', 'search')  # Default to search
 113           doc_title = data.get('title', None)
 114           
 115           if not text:
 116               return jsonify({"error": "No text provided"}), 400
 117           
 118           # Apply task-specific prompts for EmbeddingGemma
 119           if task_type == 'document':
 120               # Document format
 121               title_part = doc_title if doc_title else "none"
 122               formatted_text = f"title: {title_part} | text: {text}"
 123           else:
 124               # Query format with various tasks
 125               task_descriptions = {
 126                   'search': 'search result',
 127                   'qa': 'question answering', 
 128                   'fact': 'fact checking',
 129                   'classification': 'classification',
 130                   'clustering': 'clustering',
 131                   'similarity': 'sentence similarity',
 132                   'code': 'code retrieval'
 133               }
 134               task_desc = task_descriptions.get(task_type, 'search result')
 135               formatted_text = f"task: {task_desc} | query: {text}"
 136               
 137           # Generate embedding
 138           embedding = model.encode([formatted_text])[0]
 139           
 140           return jsonify({
 141               "embedding": embedding.tolist(),
 142               "dimension": len(embedding)
 143           })
 144       except Exception as e:
 145           logger.error(f"Embedding error: {e}")
 146           return jsonify({"error": str(e)}), 500
 147   
 148   @app.route('/embed/batch', methods=['POST'])
 149   def embed_batch():
 150       """Embed multiple texts"""
 151       try:
 152           data = request.get_json()
 153           texts = data.get('texts', [])
 154           task_type = data.get('task', 'search')  # Default to search
 155           doc_titles = data.get('titles', None)  # Optional list of titles for documents
 156           
 157           if not texts or not isinstance(texts, list):
 158               return jsonify({"error": "No texts provided or invalid format"}), 400
 159           
 160           # Apply task-specific prompts for EmbeddingGemma
 161           formatted_texts = []
 162           for i, text in enumerate(texts):
 163               if task_type == 'document':
 164                   # Document format
 165                   title = doc_titles[i] if doc_titles and i < len(doc_titles) else "none"
 166                   formatted_text = f"title: {title} | text: {text}"
 167               else:
 168                   # Query format with various tasks
 169                   task_descriptions = {
 170                       'search': 'search result',
 171                       'qa': 'question answering',
 172                       'fact': 'fact checking',
 173                       'classification': 'classification',
 174                       'clustering': 'clustering',
 175                       'similarity': 'sentence similarity',
 176                       'code': 'code retrieval'
 177                   }
 178                   task_desc = task_descriptions.get(task_type, 'search result')
 179                   formatted_text = f"task: {task_desc} | query: {text}"
 180               formatted_texts.append(formatted_text)
 181               
 182           # Generate embeddings
 183           embeddings = model.encode(formatted_texts)
 184           
 185           return jsonify({
 186               "embeddings": [emb.tolist() for emb in embeddings],
 187               "count": len(embeddings),
 188               "dimension": len(embeddings[0]) if len(embeddings) > 0 else 0
 189           })
 190       except Exception as e:
 191           logger.error(f"Batch embedding error: {e}")
 192           return jsonify({"error": str(e)}), 500
 193   
 194   if __name__ == '__main__':
 195       # Load model on startup
 196       if not load_model():
 197           exit(1)
 198       
 199       # Start server
 200       port = int(os.environ.get('PORT', 8001))
 201       app.run(host='0.0.0.0', port=port, debug=False, threaded=True)

=== End of C:\Models\ai-memory-service\embedding_service.py ===



OUTPUT EXACTLY AS SHOWN IN THE TEMPLATE BELOW.

TOKEN LIMIT: 4500

=== REQUIRED OUTPUT FORMAT ===
OUTPUT EXACTLY AS SHOWN BELOW:

Quality: {SCORE}/1000 | Status: {STATUS}

<BRIEF_ANALYSIS>
{EXECUTIVE_SUMMARY - 2-3 sentences about the code and main findings}
</BRIEF_ANALYSIS>

<TASK_CONTEXT>
Task: {USER_GOAL - what the user was trying to achieve}
Status: {COMPLETION_STATUS - how much of the task is completed}  
Next steps:
  ‚Üí {NEXT_STEP_1}
  ‚Üí {NEXT_STEP_2}
  ‚Üí {NEXT_STEP_3}
</TASK_CONTEXT>

<CRITICAL_ISSUES>
{If no critical issues, write: "No critical issues requiring immediate fix."}
{If there are issues, for each one:}
üî¥ **{CATEGORY}:** {ISSUE_TITLE}
   Problem: {PROBLEM_DESCRIPTION}
   Impact: {IMPACT_DESCRIPTION}
   Solution: {SOLUTION_DESCRIPTION}
   ```{language}
   {CODE_EXAMPLE_IF_NEEDED}
   ```
</CRITICAL_ISSUES>

<IMPROVEMENT_OPPORTUNITIES>
{For each improvement use the format:}
{üî¥ for P1 priority, üü° for P2 priority, üü¢ for P3 priority}

{PRIORITY_EMOJI} **{PRIORITY} {CATEGORY}:** {IMPROVEMENT_TITLE}
   Problem: {PROBLEM_DESCRIPTION}
   Impact: {IMPACT_DESCRIPTION}
   Solution: {SOLUTION_DESCRIPTION}
   {Add code example if needed}
</IMPROVEMENT_OPPORTUNITIES>

<WHAT_WORKS_WELL>
‚Ä¢ {POSITIVE_ASPECT_1}
‚Ä¢ {POSITIVE_ASPECT_2}
‚Ä¢ {POSITIVE_ASPECT_3}
‚Ä¢ {POSITIVE_ASPECT_4}
</WHAT_WORKS_WELL>

<SCORE_BREAKDOWN>
‚Ä¢ Functionality: {FUNC_SCORE}/300 - {FUNC_COMMENT}
‚Ä¢ Reliability: {RELIABILITY_SCORE}/200 - {RELIABILITY_COMMENT}
‚Ä¢ Maintainability: {MAINTAINABILITY_SCORE}/200 - {MAINTAINABILITY_COMMENT}
‚Ä¢ Performance: {PERFORMANCE_SCORE}/150 - {PERFORMANCE_COMMENT}
‚Ä¢ Security: {SECURITY_SCORE}/100 - {SECURITY_COMMENT}
‚Ä¢ Standards: {STANDARDS_SCORE}/50 - {STANDARDS_COMMENT}
</SCORE_BREAKDOWN>
===== CRITICAL RULES =====
Example shows English labels but YOU MUST translate them to the configured language
- Fill placeholders with actual data
- Use emojis: üî¥ üü° üü¢
- Priority: P1/P2/P3
- Max 4500 tokens
===== END RULES =====
=== END FORMAT ===


OUTPUT EXACTLY AS TEMPLATE. ANY FORMAT ALLOWED IF TEMPLATE SHOWS IT.
RESPOND IN RUSSIAN LANGUAGE.

=== END OF PROMPT ===
