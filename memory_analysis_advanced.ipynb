{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Memory Service - Advanced Analysis Notebook\n",
    "## Comprehensive System Analysis and Optimization\n",
    "\n",
    "This notebook provides advanced analysis tools for the AI Memory Service, including:\n",
    "- System performance metrics\n",
    "- Memory quality assessment\n",
    "- Embedding analysis\n",
    "- Search optimization\n",
    "- Real-time monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "MEMORY_API_URL = os.getenv('MEMORY_API_URL', 'http://127.0.0.1:8080')\n",
    "EMBEDDING_API_URL = os.getenv('EMBEDDING_API_URL', 'http://127.0.0.1:8090')\n",
    "SIMILARITY_THRESHOLD = float(os.getenv('SIMILARITY_THRESHOLD', '0.1'))\n",
    "\n",
    "print(f\"Memory API: {MEMORY_API_URL}\")\n",
    "print(f\"Embedding API: {EMBEDDING_API_URL}\")\n",
    "print(f\"Similarity Threshold: {SIMILARITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Service Health Check and Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServiceMonitor:\n",
    "    \"\"\"Monitor health and status of AI Memory services\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.services = {\n",
    "            'memory': MEMORY_API_URL,\n",
    "            'embedding': EMBEDDING_API_URL\n",
    "        }\n",
    "        self.health_history = []\n",
    "    \n",
    "    def check_service(self, name: str, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check individual service health\"\"\"\n",
    "        try:\n",
    "            start = time.time()\n",
    "            response = requests.get(f\"{url}/health\", timeout=5)\n",
    "            latency = (time.time() - start) * 1000\n",
    "            \n",
    "            return {\n",
    "                'name': name,\n",
    "                'status': 'online' if response.status_code == 200 else 'degraded',\n",
    "                'code': response.status_code,\n",
    "                'latency_ms': round(latency, 2),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'name': name,\n",
    "                'status': 'offline',\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def check_all(self) -> pd.DataFrame:\n",
    "        \"\"\"Check all services\"\"\"\n",
    "        results = []\n",
    "        for name, url in self.services.items():\n",
    "            result = self.check_service(name, url)\n",
    "            results.append(result)\n",
    "            self.health_history.append(result)\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        return df\n",
    "    \n",
    "    def visualize_health(self):\n",
    "        \"\"\"Visualize service health status\"\"\"\n",
    "        df = self.check_all()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Status pie chart\n",
    "        status_counts = df['status'].value_counts()\n",
    "        colors = {'online': 'green', 'degraded': 'orange', 'offline': 'red'}\n",
    "        axes[0].pie(status_counts.values, labels=status_counts.index, \n",
    "                   autopct='%1.0f%%', colors=[colors.get(s, 'gray') for s in status_counts.index])\n",
    "        axes[0].set_title('Service Status')\n",
    "        \n",
    "        # Latency bar chart\n",
    "        online_services = df[df['status'] == 'online']\n",
    "        if not online_services.empty:\n",
    "            axes[1].bar(online_services['name'], online_services['latency_ms'])\n",
    "            axes[1].set_ylabel('Latency (ms)')\n",
    "            axes[1].set_title('Service Latency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Run health check\n",
    "monitor = ServiceMonitor()\n",
    "health_df = monitor.visualize_health()\n",
    "display(health_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory System Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAnalyzer:\n",
    "    \"\"\"Analyze AI Memory system performance and quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api_url = MEMORY_API_URL\n",
    "        self.memories = []\n",
    "        self.stats = {}\n",
    "    \n",
    "    def fetch_memories(self, limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"Fetch memories from the system\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.api_url}/memories\", \n",
    "                                  params={'limit': limit})\n",
    "            if response.status_code == 200:\n",
    "                self.memories = response.json()\n",
    "                return self.memories\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching memories: {e}\")\n",
    "        return []\n",
    "    \n",
    "    def analyze_distribution(self):\n",
    "        \"\"\"Analyze memory distribution and patterns\"\"\"\n",
    "        if not self.memories:\n",
    "            self.fetch_memories()\n",
    "        \n",
    "        # Convert to DataFrame for analysis\n",
    "        df = pd.DataFrame(self.memories)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        self.stats = {\n",
    "            'total_memories': len(df),\n",
    "            'unique_ids': df['id'].nunique() if 'id' in df else 0,\n",
    "            'avg_content_length': df['content'].str.len().mean() if 'content' in df else 0,\n",
    "            'memory_types': df['type'].value_counts().to_dict() if 'type' in df else {},\n",
    "            'date_range': {\n",
    "                'earliest': df['timestamp'].min() if 'timestamp' in df else None,\n",
    "                'latest': df['timestamp'].max() if 'timestamp' in df else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Visualizations\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Memory types distribution\n",
    "        if 'type' in df:\n",
    "            df['type'].value_counts().plot(kind='bar', ax=axes[0, 0])\n",
    "            axes[0, 0].set_title('Memory Types Distribution')\n",
    "            axes[0, 0].set_xlabel('Type')\n",
    "            axes[0, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Content length distribution\n",
    "        if 'content' in df:\n",
    "            df['content'].str.len().hist(bins=30, ax=axes[0, 1])\n",
    "            axes[0, 1].set_title('Content Length Distribution')\n",
    "            axes[0, 1].set_xlabel('Length')\n",
    "            axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Temporal distribution\n",
    "        if 'timestamp' in df:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df.set_index('timestamp').resample('D').size().plot(ax=axes[1, 0])\n",
    "            axes[1, 0].set_title('Memories Over Time')\n",
    "            axes[1, 0].set_xlabel('Date')\n",
    "            axes[1, 0].set_ylabel('Count')\n",
    "        \n",
    "        # Similarity scores distribution (if available)\n",
    "        if 'similarity' in df:\n",
    "            df['similarity'].hist(bins=20, ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Similarity Scores Distribution')\n",
    "            axes[1, 1].set_xlabel('Similarity')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].axvline(x=SIMILARITY_THRESHOLD, color='r', \n",
    "                              linestyle='--', label=f'Threshold: {SIMILARITY_THRESHOLD}')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.stats\n",
    "\n",
    "# Analyze memory system\n",
    "analyzer = MemoryAnalyzer()\n",
    "stats = analyzer.analyze_distribution()\n",
    "print(\"\\nMemory System Statistics:\")\n",
    "print(json.dumps(stats, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embedding Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTester:\n",
    "    \"\"\"Test and analyze embedding quality\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embed_url = EMBEDDING_API_URL\n",
    "        self.test_results = []\n",
    "    \n",
    "    def generate_embedding(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Generate embedding for text\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.embed_url}/embed\",\n",
    "                json={'text': text}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                return np.array(response.json()['embedding'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "    \n",
    "    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def test_embedding_quality(self):\n",
    "        \"\"\"Test embedding quality with various text pairs\"\"\"\n",
    "        test_pairs = [\n",
    "            # Similar texts\n",
    "            (\"The cat sat on the mat\", \"A cat was sitting on a mat\", \"high\"),\n",
    "            (\"Machine learning is powerful\", \"ML is a powerful technology\", \"high\"),\n",
    "            # Different texts\n",
    "            (\"The weather is sunny\", \"I love programming\", \"low\"),\n",
    "            (\"Python is a programming language\", \"The ocean is deep\", \"low\"),\n",
    "            # Moderate similarity\n",
    "            (\"AI helps with automation\", \"Artificial intelligence automates tasks\", \"medium\"),\n",
    "            (\"Data science uses statistics\", \"Statistical analysis in data\", \"medium\")\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for text1, text2, expected in test_pairs:\n",
    "            emb1 = self.generate_embedding(text1)\n",
    "            emb2 = self.generate_embedding(text2)\n",
    "            \n",
    "            if emb1 is not None and emb2 is not None:\n",
    "                similarity = self.cosine_similarity(emb1, emb2)\n",
    "                results.append({\n",
    "                    'text1': text1[:30] + '...' if len(text1) > 30 else text1,\n",
    "                    'text2': text2[:30] + '...' if len(text2) > 30 else text2,\n",
    "                    'expected': expected,\n",
    "                    'similarity': similarity,\n",
    "                    'passed': self._check_expectation(similarity, expected)\n",
    "                })\n",
    "        \n",
    "        self.test_results = pd.DataFrame(results)\n",
    "        return self.test_results\n",
    "    \n",
    "    def _check_expectation(self, similarity: float, expected: str) -> bool:\n",
    "        \"\"\"Check if similarity meets expectation\"\"\"\n",
    "        if expected == 'high':\n",
    "            return similarity > 0.7\n",
    "        elif expected == 'low':\n",
    "            return similarity < 0.3\n",
    "        else:  # medium\n",
    "            return 0.3 <= similarity <= 0.7\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize embedding test results\"\"\"\n",
    "        if self.test_results.empty:\n",
    "            self.test_embedding_quality()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Similarity scores by expectation\n",
    "        self.test_results.boxplot(column='similarity', by='expected', ax=axes[0])\n",
    "        axes[0].set_title('Similarity Scores by Expected Category')\n",
    "        axes[0].set_xlabel('Expected Similarity')\n",
    "        axes[0].set_ylabel('Actual Similarity Score')\n",
    "        \n",
    "        # Pass/Fail distribution\n",
    "        pass_counts = self.test_results['passed'].value_counts()\n",
    "        axes[1].pie(pass_counts.values, labels=['Passed', 'Failed'], \n",
    "                   autopct='%1.0f%%', colors=['green', 'red'])\n",
    "        axes[1].set_title('Test Results')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.test_results\n",
    "\n",
    "# Test embeddings\n",
    "tester = EmbeddingTester()\n",
    "test_results = tester.visualize_results()\n",
    "display(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchOptimizer:\n",
    "    \"\"\"Optimize and analyze search performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api_url = MEMORY_API_URL\n",
    "        self.benchmark_results = []\n",
    "    \n",
    "    async def benchmark_search(self, query: str, threshold: float) -> Dict:\n",
    "        \"\"\"Benchmark a single search operation\"\"\"\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.api_url}/search\",\n",
    "                json={'query': query, 'similarity_threshold': threshold}\n",
    "            )\n",
    "            latency = (time.time() - start) * 1000\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                results = response.json()\n",
    "                return {\n",
    "                    'query': query,\n",
    "                    'threshold': threshold,\n",
    "                    'latency_ms': latency,\n",
    "                    'results_count': len(results),\n",
    "                    'status': 'success'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'threshold': threshold,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "    \n",
    "    async def run_benchmark_suite(self):\n",
    "        \"\"\"Run comprehensive search benchmarks\"\"\"\n",
    "        queries = [\n",
    "            \"machine learning\",\n",
    "            \"artificial intelligence\",\n",
    "            \"data analysis\",\n",
    "            \"neural networks\",\n",
    "            \"optimization algorithms\"\n",
    "        ]\n",
    "        \n",
    "        thresholds = [0.1, 0.2, 0.3, 0.5, 0.7]\n",
    "        \n",
    "        tasks = []\n",
    "        for query in queries:\n",
    "            for threshold in thresholds:\n",
    "                tasks.append(self.benchmark_search(query, threshold))\n",
    "        \n",
    "        results = await asyncio.gather(*tasks)\n",
    "        self.benchmark_results = pd.DataFrame(results)\n",
    "        return self.benchmark_results\n",
    "    \n",
    "    def analyze_performance(self):\n",
    "        \"\"\"Analyze search performance metrics\"\"\"\n",
    "        if self.benchmark_results.empty:\n",
    "            return None\n",
    "        \n",
    "        successful = self.benchmark_results[self.benchmark_results['status'] == 'success']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Latency by threshold\n",
    "        successful.groupby('threshold')['latency_ms'].mean().plot(\n",
    "            kind='bar', ax=axes[0, 0]\n",
    "        )\n",
    "        axes[0, 0].set_title('Average Latency by Threshold')\n",
    "        axes[0, 0].set_xlabel('Threshold')\n",
    "        axes[0, 0].set_ylabel('Latency (ms)')\n",
    "        \n",
    "        # Results count by threshold\n",
    "        successful.groupby('threshold')['results_count'].mean().plot(\n",
    "            kind='line', marker='o', ax=axes[0, 1]\n",
    "        )\n",
    "        axes[0, 1].set_title('Average Results Count by Threshold')\n",
    "        axes[0, 1].set_xlabel('Threshold')\n",
    "        axes[0, 1].set_ylabel('Results Count')\n",
    "        \n",
    "        # Latency distribution\n",
    "        successful['latency_ms'].hist(bins=20, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Latency Distribution')\n",
    "        axes[1, 0].set_xlabel('Latency (ms)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Success rate\n",
    "        success_rate = (self.benchmark_results['status'] == 'success').mean() * 100\n",
    "        axes[1, 1].text(0.5, 0.5, f'Success Rate\\n{success_rate:.1f}%', \n",
    "                       fontsize=30, ha='center', va='center')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate optimal threshold\n",
    "        optimal_threshold = self._find_optimal_threshold(successful)\n",
    "        print(f\"\\nOptimal Threshold: {optimal_threshold}\")\n",
    "        \n",
    "        return successful.describe()\n",
    "    \n",
    "    def _find_optimal_threshold(self, df: pd.DataFrame) -> float:\n",
    "        \"\"\"Find optimal threshold balancing results and performance\"\"\"\n",
    "        # Score = normalized_results - normalized_latency\n",
    "        by_threshold = df.groupby('threshold').agg({\n",
    "            'results_count': 'mean',\n",
    "            'latency_ms': 'mean'\n",
    "        })\n",
    "        \n",
    "        # Normalize to 0-1 scale\n",
    "        by_threshold['norm_results'] = (by_threshold['results_count'] - by_threshold['results_count'].min()) / \\\n",
    "                                       (by_threshold['results_count'].max() - by_threshold['results_count'].min())\n",
    "        by_threshold['norm_latency'] = (by_threshold['latency_ms'] - by_threshold['latency_ms'].min()) / \\\n",
    "                                       (by_threshold['latency_ms'].max() - by_threshold['latency_ms'].min())\n",
    "        \n",
    "        # Calculate score (maximize results, minimize latency)\n",
    "        by_threshold['score'] = by_threshold['norm_results'] - by_threshold['norm_latency']\n",
    "        \n",
    "        return by_threshold['score'].idxmax()\n",
    "\n",
    "# Run search optimization\n",
    "optimizer = SearchOptimizer()\n",
    "\n",
    "# Run async benchmark\n",
    "await optimizer.run_benchmark_suite()\n",
    "performance_stats = optimizer.analyze_performance()\n",
    "display(performance_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-time System Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SystemDashboard:\n",
    "    \"\"\"Real-time monitoring dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.monitor = ServiceMonitor()\n",
    "        self.running = False\n",
    "    \n",
    "    def create_dashboard(self):\n",
    "        \"\"\"Create interactive dashboard\"\"\"\n",
    "        from IPython.display import display, HTML\n",
    "        import ipywidgets as widgets\n",
    "        \n",
    "        # Create widgets\n",
    "        self.status_output = widgets.Output()\n",
    "        self.metrics_output = widgets.Output()\n",
    "        self.log_output = widgets.Output(layout={'height': '200px', 'overflow': 'auto'})\n",
    "        \n",
    "        # Control buttons\n",
    "        start_btn = widgets.Button(description=\"Start Monitoring\")\n",
    "        stop_btn = widgets.Button(description=\"Stop Monitoring\")\n",
    "        refresh_btn = widgets.Button(description=\"Refresh\")\n",
    "        \n",
    "        start_btn.on_click(lambda b: self.start_monitoring())\n",
    "        stop_btn.on_click(lambda b: self.stop_monitoring())\n",
    "        refresh_btn.on_click(lambda b: self.update_dashboard())\n",
    "        \n",
    "        # Layout\n",
    "        controls = widgets.HBox([start_btn, stop_btn, refresh_btn])\n",
    "        dashboard = widgets.VBox([\n",
    "            widgets.HTML(\"<h2>AI Memory Service Dashboard</h2>\"),\n",
    "            controls,\n",
    "            widgets.HTML(\"<h3>Service Status</h3>\"),\n",
    "            self.status_output,\n",
    "            widgets.HTML(\"<h3>System Metrics</h3>\"),\n",
    "            self.metrics_output,\n",
    "            widgets.HTML(\"<h3>Activity Log</h3>\"),\n",
    "            self.log_output\n",
    "        ])\n",
    "        \n",
    "        display(dashboard)\n",
    "        self.update_dashboard()\n",
    "    \n",
    "    def update_dashboard(self):\n",
    "        \"\"\"Update dashboard with latest data\"\"\"\n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            health = self.monitor.check_all()\n",
    "            display(health.style.apply(self._style_status, axis=1))\n",
    "        \n",
    "        with self.metrics_output:\n",
    "            clear_output(wait=True)\n",
    "            metrics = self._get_system_metrics()\n",
    "            display(pd.DataFrame([metrics]).T.rename(columns={0: 'Value'}))\n",
    "        \n",
    "        with self.log_output:\n",
    "            self._log(f\"Dashboard updated at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    def _style_status(self, row):\n",
    "        \"\"\"Style status rows based on health\"\"\"\n",
    "        if row['status'] == 'online':\n",
    "            return ['background-color: lightgreen'] * len(row)\n",
    "        elif row['status'] == 'degraded':\n",
    "            return ['background-color: yellow'] * len(row)\n",
    "        else:\n",
    "            return ['background-color: lightcoral'] * len(row)\n",
    "    \n",
    "    def _get_system_metrics(self) -> Dict:\n",
    "        \"\"\"Get current system metrics\"\"\"\n",
    "        return {\n",
    "            'Similarity Threshold': SIMILARITY_THRESHOLD,\n",
    "            'Memory API': MEMORY_API_URL,\n",
    "            'Embedding API': EMBEDDING_API_URL,\n",
    "            'Health Checks': len(self.monitor.health_history),\n",
    "            'Last Check': datetime.now().strftime('%H:%M:%S')\n",
    "        }\n",
    "    \n",
    "    def _log(self, message: str):\n",
    "        \"\"\"Add message to log\"\"\"\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "        print(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "    async def start_monitoring(self):\n",
    "        \"\"\"Start continuous monitoring\"\"\"\n",
    "        self.running = True\n",
    "        self._log(\"Monitoring started\")\n",
    "        \n",
    "        while self.running:\n",
    "            self.update_dashboard()\n",
    "            await asyncio.sleep(5)  # Update every 5 seconds\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop monitoring\"\"\"\n",
    "        self.running = False\n",
    "        self._log(\"Monitoring stopped\")\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = SystemDashboard()\n",
    "dashboard.create_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimization Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimization recommendations based on analysis\n",
    "def generate_recommendations():\n",
    "    \"\"\"Generate system optimization recommendations\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Check similarity threshold\n",
    "    if SIMILARITY_THRESHOLD > 0.3:\n",
    "        recommendations.append({\n",
    "            'category': 'Configuration',\n",
    "            'priority': 'High',\n",
    "            'issue': f'Similarity threshold too high ({SIMILARITY_THRESHOLD})',\n",
    "            'recommendation': 'Reduce similarity threshold to 0.1-0.2 for 768D embeddings',\n",
    "            'impact': 'Will significantly increase search recall'\n",
    "        })\n",
    "    \n",
    "    # Check service health\n",
    "    if monitor.health_history:\n",
    "        offline_services = [h for h in monitor.health_history if h.get('status') == 'offline']\n",
    "        if offline_services:\n",
    "            recommendations.append({\n",
    "                'category': 'Infrastructure',\n",
    "                'priority': 'Critical',\n",
    "                'issue': f'{len(offline_services)} service health checks failed',\n",
    "                'recommendation': 'Start offline services or check network configuration',\n",
    "                'impact': 'System cannot function without core services'\n",
    "            })\n",
    "    \n",
    "    # Check embedding quality\n",
    "    if not tester.test_results.empty:\n",
    "        failed_tests = tester.test_results[~tester.test_results['passed']]\n",
    "        if len(failed_tests) > 0:\n",
    "            recommendations.append({\n",
    "                'category': 'Model Quality',\n",
    "                'priority': 'Medium',\n",
    "                'issue': f'{len(failed_tests)} embedding tests failed',\n",
    "                'recommendation': 'Review embedding model configuration and prompts',\n",
    "                'impact': 'May affect search accuracy'\n",
    "            })\n",
    "    \n",
    "    # Performance recommendations\n",
    "    recommendations.extend([\n",
    "        {\n",
    "            'category': 'Performance',\n",
    "            'priority': 'Medium',\n",
    "            'issue': 'No caching layer detected',\n",
    "            'recommendation': 'Implement Redis cache for frequent queries',\n",
    "            'impact': 'Can reduce latency by 50-70%'\n",
    "        },\n",
    "        {\n",
    "            'category': 'Scalability',\n",
    "            'priority': 'Low',\n",
    "            'issue': 'Single-instance deployment',\n",
    "            'recommendation': 'Consider horizontal scaling with load balancer',\n",
    "            'impact': 'Improve reliability and throughput'\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # Display recommendations\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    \n",
    "    # Style by priority\n",
    "    def style_priority(val):\n",
    "        colors = {\n",
    "            'Critical': 'background-color: #ff4444; color: white',\n",
    "            'High': 'background-color: #ff8844',\n",
    "            'Medium': 'background-color: #ffdd44',\n",
    "            'Low': 'background-color: #44ff44'\n",
    "        }\n",
    "        return colors.get(val, '')\n",
    "    \n",
    "    styled = rec_df.style.applymap(style_priority, subset=['priority'])\n",
    "    \n",
    "    print(\"\\n📊 SYSTEM OPTIMIZATION RECOMMENDATIONS\\n\")\n",
    "    display(styled)\n",
    "    \n",
    "    return rec_df\n",
    "\n",
    "recommendations = generate_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all analysis results\n",
    "def export_analysis():\n",
    "    \"\"\"Export analysis results to files\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Create results directory\n",
    "    results_dir = Path(f'analysis_results_{timestamp}')\n",
    "    results_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export data\n",
    "    exports = {\n",
    "        'health_status.csv': health_df,\n",
    "        'memory_stats.json': stats,\n",
    "        'embedding_tests.csv': tester.test_results,\n",
    "        'search_benchmarks.csv': optimizer.benchmark_results,\n",
    "        'recommendations.csv': recommendations\n",
    "    }\n",
    "    \n",
    "    for filename, data in exports.items():\n",
    "        filepath = results_dir / filename\n",
    "        \n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        elif isinstance(data, dict):\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"✅ Exported: {filepath}\")\n",
    "    \n",
    "    # Generate summary report\n",
    "    report = f\"\"\"\n",
    "    AI Memory Service Analysis Report\n",
    "    Generated: {datetime.now()}\n",
    "    \n",
    "    System Status:\n",
    "    - Services Online: {len(health_df[health_df['status'] == 'online'])}/{len(health_df)}\n",
    "    - Average Latency: {health_df[health_df['status'] == 'online']['latency_ms'].mean():.2f}ms\n",
    "    \n",
    "    Memory System:\n",
    "    - Total Memories: {stats.get('total_memories', 0)}\n",
    "    - Similarity Threshold: {SIMILARITY_THRESHOLD}\n",
    "    \n",
    "    Recommendations:\n",
    "    - Critical: {len(recommendations[recommendations['priority'] == 'Critical'])}\n",
    "    - High: {len(recommendations[recommendations['priority'] == 'High'])}\n",
    "    - Medium: {len(recommendations[recommendations['priority'] == 'Medium'])}\n",
    "    - Low: {len(recommendations[recommendations['priority'] == 'Low'])}\n",
    "    \"\"\"\n",
    "    \n",
    "    report_path = results_dir / 'summary_report.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(f\"\\n📁 Analysis results exported to: {results_dir}\")\n",
    "    print(report)\n",
    "\n",
    "export_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}