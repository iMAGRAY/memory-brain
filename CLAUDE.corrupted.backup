# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Overview

AI Memory Service is a high-performance memory system for AI agents with human-cognitive-inspired architecture. It implements a three-layer memory recall system (semantic → contextual → detailed) with multiple memory types based on cognitive science.

## Key Commands

### Development
```bash
# Build the project
cargo build --release

# Run the service
cargo run --release

# Run specific binary
cargo run --bin memory-server

# Run with debug logging
RUST_LOG=debug cargo run
```

### Testing
```bash
# Run all tests
cargo test

# Run integration tests specifically
cargo test --test integration_test

# Run tests with logging
RUST_LOG=debug cargo test

# Run unit tests only
cargo test --lib
```

### Benchmarking
```bash
# Run performance benchmarks (SIMD optimization tests)
cargo bench

# Run specific benchmark
cargo bench --bench simd_benchmark
```

## Architecture Overview

### Core Components Structure

```
src/
├── main.rs           - Entry point with graceful shutdown, config validation
├── lib.rs            - Public API exports
├── memory.rs         - Main MemoryService orchestrator
├── storage.rs        - Neo4j graph database integration
├── embedding.rs      - ONNX EmbeddingGemma-300m model wrapper
├── brain.rs          - AI analysis and reasoning module  
├── cache.rs          - 3-level caching system (L1/DashMap, L2/Moka, L3/compressed)
├── simd_search.rs    - SIMD-optimized vector similarity search
├── api.rs            - Axum REST API handlers
├── config.rs         - Configuration management with validation
├── types.rs          - Core data types and error handling
├── metrics.rs        - Prometheus monitoring
└── security.rs       - Security utilities
```

### Memory System Architecture

The system implements a **three-layer recall mechanism**:

1. **Semantic Layer**: Direct content matching via 768-dimensional embeddings
2. **Contextual Layer**: Graph traversal for related memories  
3. **Detailed Layer**: Deep search with relaxed thresholds

**Memory Types** (based on cognitive science):
- `Semantic`: Facts and concepts without temporal context
- `Episodic`: Events with time/location context
- `Procedural`: How-to knowledge and step sequences
- `Working`: Active tasks and short-term goals

## Configuration

### Required Environment Variables
```bash
export NEO4J_URI=bolt://localhost:7687
export NEO4J_USER=neo4j
export NEO4J_PASSWORD=your_password
```

### Configuration Files
- `config.toml` - Main configuration (checked in project root, /etc/ai-memory/, ./config/)
- Configuration validation occurs at startup with fallback to safe defaults
- Environment variables override config file values (PORT, NEO4J_URI, RUST_ENV)

### Dependencies Setup
```bash
# Download ONNX model files (required for embedding generation)
wget https://huggingface.co/onnx-community/embeddinggemma-300m-ONNX/resolve/main/model.onnx -O models/embedding_model.onnx
wget https://huggingface.co/onnx-community/embeddinggemma-300m-ONNX/resolve/main/tokenizer.json -O models/tokenizer.json

# Start Neo4j (required for graph storage)
docker run -d \
  --name neo4j-memory \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/your_password \
  neo4j:5.0
```

## Performance Features

### SIMD Optimizations
- Vector similarity calculations use AVX2/SSE/NEON when available
- 3-8x performance improvement over scalar operations
- Benchmark with `cargo bench` to verify SIMD acceleration

### Caching Strategy
- **L1**: DashMap (lock-free concurrent HashMap) for hot data
- **L2**: Moka cache with TTL/TTI policies for warm data
- **L3**: LZ4-compressed cold storage
- Cache configuration in `config.toml` under `[cache]` section

### Key Dependencies
- `ort`: ONNX Runtime for embedding generation
- `neo4rs`: Neo4j async driver for graph storage
- `axum`: Web framework for REST API
- `moka`: High-performance caching with TTL
- `dashmap`: Lock-free concurrent HashMap
- `rayon`: Data parallelism for concurrent operations

## Testing Strategy

### Integration Tests
- Location: `tests/integration_test.rs`
- Tests full memory store/recall pipeline
- Requires Neo4j connection for graph operations

### Benchmarks
- Location: `benches/simd_benchmark.rs` 
- Tests SIMD vector search performance
- Compares scalar vs SIMD implementations

### Unit Tests
- Distributed across module files
- Test individual components in isolation
- Mock external dependencies where appropriate

## API Endpoints

- `POST /api/store` - Store new memory
- `POST /api/recall` - Search/retrieve memories  
- `PUT /api/memory/{id}` - Update existing memory
- `GET /health` - Health check endpoint
- `GET /metrics` - Prometheus metrics

## Development Notes

### Memory Service Initialization Order
1. Configuration loading with validation and fallbacks
2. Embedding service initialization (requires ONNX model files)
3. Graph storage connection (requires Neo4j)
4. AI brain and cache system setup
5. Health check before accepting requests

### Error Handling
- Uses `anyhow::Result` for application errors
- Custom `MemoryError` enum for domain-specific errors
- Graceful degradation with minimal configuration fallback

### Monitoring
- Structured logging with tracing
- Prometheus metrics at `/metrics`
- Environment-aware log levels (production/staging/development)